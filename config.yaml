# === MCP-AI v2 — PRODUCT CONFIG ===
# Run with:
#   python -m mcp_ai.main --config ./mcp_ai/config.yaml
#
# Design:
# • FAST pass on everything (cheap) → choose DEEP (expensive) via auto-triggers.
# • Prefer LOCAL LLMs; stage hard cases for CLOUD with explicit approval.
# • Emit clean catalog (JSONL + CSV) per-run + global rollup. Structured logs.

# ───────────────────────────── Runtime mode ─────────────────────────────
mode_selector: prompt
scan_profile: deep

paths:
  roots:
    - "/Users/vamsi_mure/Documents/mcp_ai-v2/mcp_ai/input_files/SampleFiles"
  exclude_globs:
    - "**/.git/**"
    - "**/tmp/**"
    - "**/__pycache__/**"

profiles:
  basic:
    text_extract: true
    ocr: false
    expand_archives: false
    include_types: ["pdf","docx","txt","csv","xlsx","pptx","html"]
    max_file_mb: 50

  standard:
    text_extract: true
    ocr: true
    expand_archives: true
    include_types: ["pdf","docx","txt","csv","xlsx","html","pptx"]
    max_file_mb: 100

  deep:
    text_extract: true
    ocr: true
    expand_archives: true
    include_types: ["*"]
    max_file_mb: 500
    timebox_ocr_s: 30

    detect_language: true
    collect_exif: true
    table_detection: "light"
    text_quality_probe: true

    parse_pdf_order: ["pymupdf", "pdfminer", "pdfplumber", "ocr"]

    # A/V transcription cascade
    av_transcribe_order: ["faster_whisper", "whisper", "demo"]
    av_max_seconds: 0            # 0 = full file; set e.g. 600 to cap at 10 min for speed
    av_chunk_seconds: 600        # split long audio into 10-minute chunks
    av_lang_probe_seconds: 60    # quick probe to guess language

    # OCR cascade
    ocr_engine_order: ["easyocr", "tesseract", "rapidocr", "paddle", "azure"]
    allow_cloud_ocr: false
    ocr_lang: "eng"
    ocr_dpi: 144
    ocr_max_pages: 6

# ───────────────────────────── Type policies (per-extension behavior) ──────────
type_policies:
  skip: [".iso", ".img", ".bin", ".ds_store"]          # Completely skip (no extraction)
  metadata_only: [".psd", ".ai", ".mod"]               # Heavy/unknown formats
  try_text_then_metadata_only: [".pptx", ".xlsx", ".zip"]
  prefer_ocr: [".pdf", ".tiff", ".png", ".jpg", ".jpeg"]

# NOTE: Do NOT include audio/video in metadata_only if you want transcripts:
#   Audio: .mp3, .wav, .m4a, .aac, .flac
#   Video: .mp4, .mov, .mkv

# ───────────────────────────── Domain hints (helps LLM/heuristics) ─────────────
domain:
  detect: hybrid
  list: [finance, legal, hr, it, marketing, engineering, operations, healthcare, education, general]
  hints:
    finance:  ["invoice","po","ledger","balance sheet","tax","gst","amount due","net 30","PO"]
    legal:    ["contract","agreement","nda","clause","indemnification","governing law","termination"]
    hr:       ["employee","payroll","benefits","recruiting","onboarding"]
    it:       ["server","api","database","kubernetes","terraform","endpoint","log"]

# ───────────────────────────── AI enrichment (FAST/DEEP) ───────────────────────
ai:
  mode: local                    # none | local | cloud
  provider: ollama               # ollama | azure
  fast_pass: true                # run FAST on every doc

  deep:
    policy: auto                 # auto | always | never
    auto_triggers:
      low_confidence_below: 0.90
      contains_pii: true
      high_value_flag: true
    sampling: smart              # smart | full
    pii_redaction: true
    max_chars_fast: 6000
    max_chars_local: 60000
    max_chars_cloud: 120000

  # Local (Ollama) — existing config (kept)
  local:
    endpoint: "http://127.0.0.1:11434"
    fast_model: "mistral:7b-instruct"
    deep_model: "qwen2.5:14b-instruct"
    timeout_s: 200
    retries: 3
    backoff_s: 1.2

    # NEW: faster-whisper tuning (kept)
    faster_whisper:
      model: "base"
      device: "cpu"
      compute_type: "int8"
      language: null
      vad_filter: true
      beam_size: 1
      no_speech_threshold: 0.2

  # ---------- UI compatibility bridge ----------
  # These keys make the Streamlit dashboard auto-configure itself
  local:
    endpoint: "http://127.0.0.1:11434"   # (duplicated above for clarity)
    fast_model: "mistral:7b-instruct"
    deep_model: "qwen2.5:14b-instruct"
    timeout_s: 200
    retries: 3
    backoff_s: 1.2
    faster_whisper:
      model: "base"
      device: "cpu"
      compute_type: "int8"
      language: null
      vad_filter: true
      beam_size: 1
      no_speech_threshold: 0.2

    # 👇 NEW sub-block the Streamlit reads
    ollama:
      url: "http://127.0.0.1:11434"
      # pick your preferred default model for Q&A in the UI:
      model: "llama3.1"
      embed_model: "nomic-embed-text"

  cloud:
    enabled: false               # UI will default to local unless you flip this to true
    provider: "azure-openai"     # normalized for the UI: "azure-openai" | "openai" | "none"
    azure:
      api_key: ""                # required if enabled && provider=azure-openai
      endpoint: "https://YOUR-RESOURCE.openai.azure.com"
      deployment: "gpt-4o-mini"
      embed_deployment: "text-embedding-3-small"
    openai:
      api_key: ""
      model: "gpt-4o-mini"
      embed_model: "text-embedding-3-small"

# ───────────────────────────── Cloud (Azure OpenAI) — pipeline (kept) ──────────
cloud:
  provider: "azure"
  endpoint: "https://YOUR-RESOURCE.openai.azure.com"
  api_version: "2024-08-01-preview"
  deployment_fast: "gpt-4o-mini"
  deployment_deep: "gpt-4o"
  timeout_s: 20
  pricing:
    fast_input_per_1k: 0.0005
    fast_output_per_1k: 0.0015
    deep_input_per_1k: 0.0030
    deep_output_per_1k: 0.0060

# Optional: Azure Document Intelligence (OCR) — used only if allow_cloud_ocr=true
azure:
  document_intelligence:
    endpoint_env: "AZURE_FORMREC_ENDPOINT"
    key_env: "AZURE_FORMREC_KEY"

# ───────────────────────────── Safety & demo ───────────────────────────────────
safety:
  allow_outbound_network: false
  pii_redaction: true

demo:
  enabled: true
  fixture_dir: "./mcp_ai/fixtures"
  simulate_latency_ms: 150

# ───────────────────────────── Budgets / archival ──────────────────────────────
budgets:
  run_usd: 25.0
  per_file_usd: 0.03

archival:
  stale_years: 6

# ───────────────────────────── Concurrency / limits ────────────────────────────
limits:
  max_concurrency_scan: 64
  max_concurrency_extract: 8

# ───────────────────────────── Storage / outputs ───────────────────────────────
storage:
  out_root: "./mcp_ai/output_files"
  per_run_outputs: true
  catalog_jsonl: "./mcp_ai/output_files/metadata_catalog.jsonl"
  catalog_csv: "./mcp_ai/output_files/metadata_catalog.csv"
  state_db: "./mcp_ai/output_files/state.sqlite"
  cloud_stage_dir: "./mcp_ai/output_files/cloud_stage"

# ───────────────────────────── Logging ─────────────────────────────────────────
logging:
  level: WARNING
  to_file: true
  file_path: "./mcp_ai/output_files/mcp_ai.log"
  rotate_bytes: 10485760
  backups: 2

# ───────────────────────────── One-click Scan for UI ───────────────────────────
scan:
  # This is what the Streamlit "⚙️ Scan" page will run.
  # Adjust to your CLI entry-point if different.
  command: "python -m mcp_ai.main --config ./mcp_ai/config.yaml"