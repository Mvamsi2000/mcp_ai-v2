# === MCP-AI v2 — PRODUCT CONFIG ===
# Run:
#   python -m mcp_ai.main --config ./mcp_ai/config.yaml
#
# Philosophy:
# - FAST pass (cheap) on everything → decide when to run DEEP (expensive).
# - Use local LLMs by default; only stage hard files to cloud with client approval.
# - Emit clean catalog (JSONL + CSV). Keep logs structured and rotated.

# ───────────────────────────── Runtime mode ─────────────────────────────
mode_selector: prompt          # placeholder for future CLI-driven modes
scan_profile: deep             # which profile from `profiles:` to apply

# ───────────────────────────── Scope ─────────────────────────────
paths:
  roots:
    - "/Users/vamsi_mure/Documents/mcp_ai-v2/mcp_ai/input_files/SampleFiles"
  exclude_globs:
    - "**/.git/**"
    - "**/tmp/**"
    - "**/__pycache__/**"

# ───────────────────────────── Profiles (extraction) ─────────────────────────────
profiles:
  basic:
    text_extract: true
    ocr: false
    expand_archives: false
    include_types: ["pdf","docx","txt","csv","xlsx","pptx","html"]
    max_file_mb: 50

  standard:
    text_extract: true
    ocr: true
    expand_archives: true
    include_types: ["pdf","docx","txt","csv","xlsx","html","pptx"]
    max_file_mb: 100

  deep:
    # --- core toggles ---
    text_extract: true
    ocr: true
    expand_archives: true
    include_types: ["*"]         # accept everything; type_policies gate behavior
    max_file_mb: 500
    timebox_ocr_s: 40

    # --- quality & enrichment knobs ---
    detect_language: true
    collect_exif: true
    table_detection: "light"      # off | light (placeholder)
    text_quality_probe: true

    # --- PDF parsing cascade (adaptive) ---
    parse_pdf_order: ["pymupdf", "pdfminer", "pdfplumber", "ocr"]

    # --- OCR cascade (adaptive) ---
    # Azure is only tried if allow_cloud_ocr=true AND Azure env vars exist.
    ocr_engine_order: ["easyocr", "tesseract", "rapidocr", "paddle", "azure"]
    allow_cloud_ocr: false
    ocr_lang: "eng"
    ocr_dpi: 144
    ocr_max_pages: 6

    # --- A/V transcription cascade (adaptive) ---
    av_transcribe_order: ["demo", "faster_whisper", "whisper"]

# ───────────────────────────── Type policies (what to do per extension) ─────────────────────────────
type_policies:
  # Completely skip these (no extraction)
  skip: [".iso", ".img", ".bin", ".ds_store"]
  # Metadata-only for heavy binaries/design/media
  metadata_only: [".psd", ".ai", ".mod", ".mp4", ".mov", ".mkv"]
  # Try text first; if empty, keep metadata-only
  try_text_then_metadata_only: [".pptx", ".xlsx", ".zip"]
  # Prefer OCR (PDF still attempts text parse before OCR)
  prefer_ocr: [".pdf", ".tiff", ".png", ".jpg", ".jpeg"]

# ───────────────────────────── Domain hints ─────────────────────────────
domain:
  detect: hybrid
  list: [finance, legal, hr, it, marketing, engineering, operations, healthcare, education, general]
  hints:
    finance:  ["invoice","po","ledger","balance sheet","tax","gst","amount due","net 30","PO"]
    legal:    ["contract","agreement","nda","clause","indemnification","governing law","termination"]
    hr:       ["employee","payroll","benefits","recruiting","onboarding"]
    it:       ["server","api","database","kubernetes","terraform","endpoint","log"]

# ───────────────────────────── AI enrichment ─────────────────────────────
ai:
  mode: local                  # none | local | cloud
  provider: ollama             # ollama | azure
  fast_pass: true

  deep:
    # How to run DEEP:
    # - auto: run deep only when triggers fire (recommended)
    # - always: run deep on every file
    # - never: never run deep
    policy: auto               # auto | always | never

    # Auto triggers for 'auto' policy
    auto_triggers:
      low_confidence_below: 0.75   # if FAST confidence < 0.75 -> deep
      contains_pii: false           # if FAST flags PII (or regex detects) -> deep
      high_value_flag: false        # if caller passes high_value=True -> deep

    sampling: smart                # smart | full
    confidence_threshold: 0.75     # legacy compatibility
    pii_redaction: true
    max_chars_fast: 6000
    max_chars_local: 60000
    max_chars_cloud: 120000

  # Local (Ollama)
  local:
    endpoint: "http://127.0.0.1:11434"
    fast_model: "mistral:7b-instruct"
    deep_model: "qwen2.5:14b-instruct"
    timeout_s: 200
    # retry policy used by llm_agent
    retries: 3
    backoff_s: 1.2

# Cloud (Azure OpenAI) — used only if ai.mode=cloud
cloud:
  provider: "azure"
  endpoint: "https://YOUR-RESOURCE.openai.azure.com"
  api_version: "2024-08-01-preview"
  deployment_fast: "gpt-4o-mini"
  deployment_deep: "gpt-4o"
  timeout_s: 20
  pricing:
    fast_input_per_1k: 0.0005
    fast_output_per_1k: 0.0015
    deep_input_per_1k: 0.0030
    deep_output_per_1k: 0.0060

# Optional: Azure Document Intelligence (OCR) — only used if allow_cloud_ocr=true
azure:
  document_intelligence:
    endpoint_env: "AZURE_FORMREC_ENDPOINT"
    key_env: "AZURE_FORMREC_KEY"

# ───────────────────────────── Safety / demo ─────────────────────────────
safety:
  allow_outbound_network: false   # keep OFF for local-only runs
  pii_redaction: true

demo:
  enabled: true
  fixture_dir: "./mcp_ai/fixtures"
  simulate_latency_ms: 150

# ───────────────────────────── Budgets / archival ─────────────────────────────
budgets:
  run_usd: 25.0
  per_file_usd: 0.03

archival:
  stale_years: 6

# ───────────────────────────── Concurrency / limits ─────────────────────────────
limits:
  max_concurrency_scan: 64
  max_concurrency_extract: 8

# ───────────────────────────── Storage / outputs ─────────────────────────────
storage:
  out_root: "./mcp_ai/output_files"
  per_run_outputs: true
  catalog_jsonl: "./mcp_ai/output_files/metadata_catalog.jsonl"
  catalog_csv: "./mcp_ai/output_files/metadata_catalog.csv"
  state_db: "./mcp_ai/output_files/state.sqlite"
  cloud_stage_dir: "./mcp_ai/output_files/cloud_stage"   # files suggested for cloud deep

# ───────────────────────────── Logging ─────────────────────────────
logging:
  level: INFO                # DEBUG | INFO | WARNING | ERROR
  to_file: true
  file_path: "./mcp_ai/output_files/mcp_ai.log"
  rotate_bytes: 10485760     # 10 MB
  backups: 3